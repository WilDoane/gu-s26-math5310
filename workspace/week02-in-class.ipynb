{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170f8d6-900a-4ce9-84e0-05ed38d7ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load the CSV\n",
    "# Source: https://github.com/plotly/datasets/blob/master/auto-mpg.csv\n",
    "df = pd.read_csv(\"data/auto-mpg.csv\")\n",
    "\n",
    "# 2) Look for columns that are \"text\" but mostly contain numbers\n",
    "for col in df.columns:\n",
    "    # If pandas thinks it's text (object), it *might* really be numeric\n",
    "    if df[col].dtype == \"object\":\n",
    "        # Try converting to numbers (anything that can't convert becomes NaN)\n",
    "        as_num = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Count how many non-empty values exist, and how many become numbers\n",
    "        non_missing = df[col].notna().sum()\n",
    "        numeric_count = as_num.notna().sum()\n",
    "\n",
    "        # If most of the non-missing values convert to numbers, flag it\n",
    "        if non_missing > 0 and numeric_count / non_missing >= 0.9:\n",
    "            print(f\"Column '{col}' might be numeric but loaded as text.\")\n",
    "            print(\"  examples:\", df[col].dropna().head(5).tolist())\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"Column '{col}' appears to be numeric.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cffb72-67db-45a6-b314-bfb029cf2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5528b8-9cbc-48d0-bef3-d109df1d2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training (70%) and test (30%) sets\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.30,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Print dimensions of each set\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3a850-14a5-4d3f-bf5d-509276a61d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TRAINING SET ----\n",
    "print(\"Training set missing values (before):\")\n",
    "print(train_df.isna().sum())\n",
    "\n",
    "# Fill missing values with column medians\n",
    "train_df = train_df.fillna(train_df.median(numeric_only=True))\n",
    "\n",
    "print(\"\\nTraining set missing values (after):\")\n",
    "print(train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d98031-2ba8-452a-839e-f1d4fd6e463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TEST SET ----\n",
    "print(\"\\nTest set missing values (before):\")\n",
    "print(test_df.isna().sum())\n",
    "\n",
    "# Fill missing values with column medians\n",
    "test_df = test_df.fillna(test_df.median(numeric_only=True))\n",
    "\n",
    "print(\"\\nTest set missing values (after):\")\n",
    "print(test_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64237208-431f-492a-8733-a97081d70f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7d941-ff00-45a1-a2cd-5639b8228db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# -------------------------------\n",
    "# Normalize continuous features\n",
    "# -------------------------------\n",
    "\n",
    "# Choose continuous columns (example ones)\n",
    "continuous_cols = [\"horsepower\", \"weight\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data, transform both\n",
    "train_df[continuous_cols] = scaler.fit_transform(train_df[continuous_cols])\n",
    "test_df[continuous_cols] = scaler.transform(test_df[continuous_cols])\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a51fe-d383-49f7-940f-24a1ff3f5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Add polynomial features (squared)\n",
    "# --------------------------------\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Apply only to horsepower for simplicity\n",
    "## learn the poly model from training set\n",
    "horsepower_train_poly = poly.fit_transform(train_df[[\"horsepower\"]])\n",
    "\n",
    "## aply the same transform to the test set\n",
    "horsepower_test_poly  = poly.transform(test_df[[\"horsepower\"]])\n",
    "\n",
    "# Convert back to DataFrames\n",
    "poly_feature_names = poly.get_feature_names_out([\"horsepower\"])\n",
    "\n",
    "horsepower_train_poly = pd.DataFrame(\n",
    "    horsepower_train_poly,\n",
    "    columns=poly_feature_names,\n",
    "    index=train_df.index\n",
    ")\n",
    "\n",
    "horsepower_test_poly = pd.DataFrame(\n",
    "    horsepower_test_poly,\n",
    "    columns=poly_feature_names,\n",
    "    index=test_df.index\n",
    ")\n",
    "\n",
    "# Add squared term back to original data\n",
    "train_df[\"horsepower_squared\"] = horsepower_train_poly[\"horsepower^2\"]\n",
    "test_df[\"horsepower_squared\"]  = horsepower_test_poly[\"horsepower^2\"]\n",
    "\n",
    "# Show result\n",
    "print(\"Training columns:\", train_df.columns.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d181c81f-25ee-4e2d-93bf-825f16cad6e3",
   "metadata": {},
   "source": [
    "# Let's start over and build a full model using Kfold approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7eccbf-59b7-4465-8561-54d4c7cc7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/auto-mpg.csv\")\n",
    "\n",
    "# Simple cleanup: drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"mpg\"])\n",
    "y = df[\"mpg\"]\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    rmse_scores.append(rmse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Average metrics\n",
    "print(\"Average R-squared:\", np.mean(r2_scores))\n",
    "print(\"Average RMSE:\", np.mean(rmse_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da3a29-4bc6-4810-9d38-ec2380c201e4",
   "metadata": {},
   "source": [
    "# And again, let's start fresh adding some automated model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f45fbf-b786-4ab5-9056-c881ce98e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# -----------------------\n",
    "# Load and prepare data\n",
    "# -----------------------\n",
    "df = pd.read_csv(\"data/auto-mpg.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop(columns=[\"mpg\"])\n",
    "y = df[\"mpg\"]\n",
    "\n",
    "# Split into training (70%) and test (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=seed\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Pick best polynomial degree using 5-fold CV on TRAINING only\n",
    "# -----------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def cv_scores_for_degree(degree):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "    r2_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    # We'll wrap X_poly in a DataFrame so .iloc works easily\n",
    "    X_poly = pd.DataFrame(X_poly)\n",
    "\n",
    "    for tr_idx, val_idx in kf.split(X_poly):\n",
    "        X_tr = X_poly.iloc[tr_idx]\n",
    "        X_val = X_poly.iloc[val_idx]\n",
    "        y_tr = y_train.iloc[tr_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        r2_list.append(r2_score(y_val, preds))\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    return np.mean(r2_list), np.mean(rmse_list)\n",
    "\n",
    "results = {}\n",
    "for degree in [1, 2]:\n",
    "    avg_r2, avg_rmse = cv_scores_for_degree(degree)\n",
    "    results[degree] = (avg_r2, avg_rmse)\n",
    "    print(f\"Degree {degree} CV -> Avg R²: {avg_r2:.4f}, Avg RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "# Choose best degree (highest R²; if tie, lowest RMSE)\n",
    "best_degree = max(results.keys(), key=lambda d: (results[d][0], -results[d][1]))\n",
    "print(\"\\nBest polynomial degree:\", best_degree)\n",
    "\n",
    "# -----------------------\n",
    "# Train FINAL model on full training set using best degree\n",
    "# -----------------------\n",
    "poly = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)   # IMPORTANT: transform only\n",
    "\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# -----------------------\n",
    "# Evaluate on test set\n",
    "# -----------------------\n",
    "test_preds = final_model.predict(X_test_poly)\n",
    "\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "\n",
    "print(\"\\nTest set results:\")\n",
    "print(\"  R-squared:\", test_r2)\n",
    "print(\"  RMSE     :\", test_rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5a6f0-071f-43c7-a502-d2b16ffe8927",
   "metadata": {},
   "source": [
    "# Building a Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce66d6d-42a4-4353-bc5c-cbee3c49defd",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d64e3-44ed-4301-b451-556ab71fd752",
   "metadata": {},
   "source": [
    "The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable that indicates whether an applicant has ultimately defaulted or has been severely delinquent. This adverse outcome occurred in 1,189 cases (20 percent). 12 input variables were registered for each applicant.\n",
    "\n",
    "**BAD**: 1 = Client defaulted on loan, 0 = loan repaid\n",
    "\n",
    "**LOAN**: Amount of loan approved.\n",
    "\n",
    "**MORTDUE**: Amount due on the existing mortgage.\n",
    "\n",
    "**VALUE**: Current value of the property.\n",
    "\n",
    "**REASON**: Reason for the loan request. (HomeImp = home improvement, DebtCon= debt consolidation which means taking out a new loan to pay off other liabilities and consumer debts)\n",
    "\n",
    "**JOB**: The type of job that loan applicant has such as manager, self, etc.\n",
    "\n",
    "**YOJ**: Years at present job.\n",
    "\n",
    "**DEROG**: Number of major derogatory reports (which indicates a serious delinquency or late payments).\n",
    "\n",
    "**DELINQ**: Number of delinquent credit lines (a line of credit becomes delinquent when a borrower does not make the minimum required payments 30 to 60 days past the day on which the payments were due).\n",
    "\n",
    "**CLAGE**: Age of the oldest credit line in months.\n",
    "\n",
    "**NINQ**: Number of recent credit inquiries.\n",
    "\n",
    "**CLNO**: Number of existing credit lines.\n",
    "\n",
    "**DEBTINC**: Debt-to-income ratio (all your monthly debt payments divided by your gross monthly income. This number is one way lenders measure your ability to manage the monthly payments to repay the money you plan to borrow.\n",
    "\n",
    "Source: <https://github.com/NishadKhudabux/Loan-Default-Prediction/blob/main/Loan%20Default%20Prediction%20-%20Model%20Building.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da988d-497c-434e-a254-4857ae2d8563",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a1a63-7465-4dd5-b175-e0b35f5b3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('data/hmeq-loan-defaults.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7697233-b376-4ead-ac2b-79da37a8ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle missing values\n",
    "\n",
    "# Fill missing numerical values with the median\n",
    "for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    data[column] = data[column].fillna(data[column].median())\n",
    "    \n",
    "# Fill missing categorical values with the mode\n",
    "data['REASON'] = data['REASON'].fillna(data['REASON'].mode()[0])\n",
    "data['JOB'] = data['JOB'].fillna(data['JOB'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dd6d2-0f60-4a5d-b871-d7da071d1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert categorical variables to numeric using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data['REASON'] = label_encoder.fit_transform(data['REASON'])\n",
    "data['JOB'] = label_encoder.fit_transform(data['JOB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7eb284-e56a-4a53-b92d-c9c49279f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split data into features and target variable\n",
    "X = data.drop('BAD', axis=1)  # Features\n",
    "y = data['BAD']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6f6cf-32f0-4e6f-bdbf-c20364606b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233fc9c-d5e1-4806-b8e4-6eb6361cd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc82a6-092b-478e-9504-93e481fdf781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c62ec-1248-4def-a3dd-802883950e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e49ef-d7cc-4a7c-b81b-0de307747ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab17c44-c150-4cdb-8123-4dc36b32d70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
