{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b591c44",
   "metadata": {},
   "source": [
    "\n",
    "# In-class Activity: Feedforward Neural Network (From Scratch) + Framework Comparison (PyTorch)\n",
    "\n",
    "This notebook mirrors the homework expectations:\n",
    "\n",
    "- **2D input**\n",
    "- **Two hidden layers** with **sigmoid** activations\n",
    "- **2-class output** with **softmax**\n",
    "- **No bias terms**\n",
    "- **SGD training** (single-example updates)\n",
    "- **Track and plot training loss**\n",
    "- **Report final weights**\n",
    "_toggle into PyTorch and compare with precision / recall / F1 on the test set._\n",
    "\n",
    "> Data files are expected at: `data/week04/train.csv` and `data/week04/test.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f17f7",
   "metadata": {},
   "source": [
    "\n",
    "## What you should learn today\n",
    "\n",
    "By the end of this activity, you should be able to:\n",
    "\n",
    "1. Implement a forward pass with clear **matrix shapes**\n",
    "2. Implement backprop for a small network **without relying on a framework**\n",
    "3. Run **SGD** and track loss over time\n",
    "4. Diagnose common training pathologies (e.g., predicting only one class)\n",
    "5. Compare a scratch implementation to a **PyTorch baseline** under the same constraints\n",
    "\n",
    "### Suggested workflow during class\n",
    "- Run the notebook once top-to-bottom.\n",
    "- Then experiment with **learning rate**, **epochs**, **hidden sizes**, and **initialization seed**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce136dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7600e",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load data\n",
    "\n",
    "We load CSVs from `data/week04/`.  \n",
    "If the files are not present, we generate a simple synthetic 2D dataset so that the notebook still runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_or_make_data(train_path=\"data/week04/train.csv\", test_path=\"data/week04/test.csv\", seed=0):\n",
    "    train_path = Path(train_path)\n",
    "    test_path = Path(test_path)\n",
    "\n",
    "    if train_path.exists() and test_path.exists():\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df  = pd.read_csv(test_path)\n",
    "\n",
    "        # Assumption: last column is the label\n",
    "        X_train = train_df.iloc[:, :-1].to_numpy(dtype=np.float64)\n",
    "        y_train = train_df.iloc[:,  -1].to_numpy()\n",
    "        X_test  = test_df.iloc[:,  :-1].to_numpy(dtype=np.float64)\n",
    "        y_test  = test_df.iloc[:,   -1].to_numpy()\n",
    "\n",
    "        # If labels are one-hot, convert to indices\n",
    "        if y_train.ndim > 1 and y_train.shape[1] > 1:\n",
    "            y_train = np.argmax(y_train, axis=1)\n",
    "            y_test  = np.argmax(y_test, axis=1)\n",
    "\n",
    "        y_train = y_train.astype(int)\n",
    "        y_test  = y_test.astype(int)\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, \"loaded_csv\"\n",
    "\n",
    "    # ---- fallback synthetic dataset ----\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_train, n_test = 400, 200\n",
    "\n",
    "    mu0 = np.array([-1.0, -1.0])\n",
    "    mu1 = np.array([ 1.0,  1.0])\n",
    "    cov = np.array([[0.5, 0.0],\n",
    "                    [0.0, 0.5]])\n",
    "\n",
    "    X0 = rng.multivariate_normal(mu0, cov, size=n_train//2)\n",
    "    X1 = rng.multivariate_normal(mu1, cov, size=n_train//2)\n",
    "    X_train = np.vstack([X0, X1])\n",
    "    y_train = np.hstack([np.zeros(n_train//2, dtype=int),\n",
    "                         np.ones(n_train//2, dtype=int)])\n",
    "\n",
    "    X0t = rng.multivariate_normal(mu0, cov, size=n_test//2)\n",
    "    X1t = rng.multivariate_normal(mu1, cov, size=n_test//2)\n",
    "    X_test = np.vstack([X0t, X1t])\n",
    "    y_test = np.hstack([np.zeros(n_test//2, dtype=int),\n",
    "                        np.ones(n_test//2, dtype=int)])\n",
    "\n",
    "    # shuffle\n",
    "    idx = rng.permutation(n_train)\n",
    "    X_train, y_train = X_train[idx], y_train[idx]\n",
    "    idt = rng.permutation(n_test)\n",
    "    X_test, y_test = X_test[idt], y_test[idt]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, \"synthetic\"\n",
    "\n",
    "X_train, y_train, X_test, y_test, data_mode = load_or_make_data()\n",
    "print(\"data_mode:\", data_mode)\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"classes:\", np.unique(y_train))\n",
    "print(\"X_test :\", X_test.shape,  \"y_test :\", y_test.shape,  \"classes:\", np.unique(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick visualization (assumes 2D features, as in the homework)\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, s=18)\n",
    "plt.title(\"Training data (colored by class)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7002a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Activation + Softmax (numerically stable)\n",
    "\n",
    "Key ideas:\n",
    "- Clamp sigmoid input to prevent overflow in `exp()`\n",
    "- Subtract `max(logits)` before softmax so that `exp()` is stable\n",
    "\n",
    "We also use one-hot encoding for the loss and for the backprop convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6af031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=-1, keepdims=True)\n",
    "    expz = np.exp(z)\n",
    "    return expz / np.sum(expz, axis=-1, keepdims=True)\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    out = np.zeros((y.shape[0], num_classes), dtype=np.float64)\n",
    "    out[np.arange(y.shape[0]), y] = 1.0\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d77b8",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Cross-entropy loss\n",
    "\n",
    "For a 2-class softmax output, we use:\n",
    "\n",
    "$\n",
    "\\mathcal{L} = -\\sum_{c} y_c \\log(\\hat{p}_c)\n",
    "$\n",
    "\n",
    "Where `y` is one-hot and `p` are softmax probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy_loss(probs, y_onehot, eps=1e-12):\n",
    "    probs = np.clip(probs, eps, 1.0)\n",
    "    return -np.sum(y_onehot * np.log(probs), axis=1)  # per-sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d481c7",
   "metadata": {},
   "source": [
    "\n",
    "## 4) From-scratch network (no biases)\n",
    "\n",
    "Architecture:\n",
    "- Input (2) → Hidden1 (`h1`, sigmoid) → Hidden2 (`h2`, sigmoid) → Output (2, softmax)\n",
    "\n",
    "**No bias terms** means every layer is only a matrix multiplication: `X @ W`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ffb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoHiddenNN:\n",
    "    def __init__(self, d_in=2, h1=3, h2=3, d_out=2, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W1 = rng.normal(0, 0.2, size=(d_in, h1)).astype(np.float64)\n",
    "        self.W2 = rng.normal(0, 0.2, size=(h1, h2)).astype(np.float64)\n",
    "        self.W3 = rng.normal(0, 0.2, size=(h2, d_out)).astype(np.float64)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z1 = X @ self.W1\n",
    "        A1 = sigmoid(Z1)\n",
    "\n",
    "        Z2 = A1 @ self.W2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        Z3 = A2 @ self.W3\n",
    "        P  = softmax(Z3)\n",
    "\n",
    "        cache = {\"X\": X, \"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2, \"Z3\": Z3, \"P\": P}\n",
    "        return P, cache\n",
    "\n",
    "    def backward(self, cache, y_onehot):\n",
    "        X  = cache[\"X\"]\n",
    "        A1 = cache[\"A1\"]\n",
    "        A2 = cache[\"A2\"]\n",
    "        P  = cache[\"P\"]\n",
    "        Z1 = cache[\"Z1\"]\n",
    "        Z2 = cache[\"Z2\"]\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Softmax + CrossEntropy gives: dZ3 = (P - Y)/N\n",
    "        dZ3 = (P - y_onehot) / N\n",
    "        dW3 = A2.T @ dZ3\n",
    "\n",
    "        dA2 = dZ3 @ self.W3.T\n",
    "        s2 = sigmoid(Z2)\n",
    "        dZ2 = dA2 * (s2 * (1.0 - s2))\n",
    "        dW2 = A1.T @ dZ2\n",
    "\n",
    "        dA1 = dZ2 @ self.W2.T\n",
    "        s1 = sigmoid(Z1)\n",
    "        dZ1 = dA1 * (s1 * (1.0 - s1))\n",
    "        dW1 = X.T @ dZ1\n",
    "\n",
    "        return {\"dW1\": dW1, \"dW2\": dW2, \"dW3\": dW3}\n",
    "\n",
    "    def predict(self, X):\n",
    "        P, _ = self.forward(X)\n",
    "        return np.argmax(P, axis=1)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {\"W1\": self.W1.copy(), \"W2\": self.W2.copy(), \"W3\": self.W3.copy()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969d651",
   "metadata": {},
   "source": [
    "\n",
    "## 5) SGD training loop (single-example updates)\n",
    "\n",
    "We do **stochastic gradient descent** by:\n",
    "1. Shuffling examples each epoch\n",
    "2. For each example: forward → loss → backward → update weights\n",
    "\n",
    "We log the loss **every step** so you can plot it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77109827-682e-48b2-8c97-5b67596c414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(model, X, y, lr=0.1, epochs=30, seed=0, shuffle=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    losses = []\n",
    "\n",
    "    K = len(np.unique(y))\n",
    "    n = X.shape[0]\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        idx = rng.permutation(n) if shuffle else np.arange(n)\n",
    "\n",
    "        for i in idx:\n",
    "            Xi = X[i:i+1]           # (1,2)\n",
    "            yi = y[i:i+1]           # (1,)\n",
    "            Yi = one_hot(yi, K)     # (1,2)\n",
    "\n",
    "            P, cache = model.forward(Xi)\n",
    "            loss = cross_entropy_loss(P, Yi).item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            grads = model.backward(cache, Yi)\n",
    "\n",
    "            model.W1 -= lr * grads[\"dW1\"]\n",
    "            model.W2 -= lr * grads[\"dW2\"]\n",
    "            model.W3 -= lr * grads[\"dW3\"]\n",
    "\n",
    "        # small progress report\n",
    "        if (ep + 1) % max(1, epochs // 5) == 0:\n",
    "            yhat = model.predict(X)\n",
    "            acc = (yhat == y).mean()\n",
    "            print(f\"epoch {ep+1:3d}/{epochs} | last_step_loss={losses[-1]:.4f} | train_acc={acc:.3f}\")\n",
    "\n",
    "    return np.array(losses, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e6643",
   "metadata": {},
   "source": [
    "\n",
    "### Try-it-now: adjust parameters\n",
    "\n",
    "Start with the defaults, then experiment:\n",
    "\n",
    "- `lr` (learning rate): try **0.01, 0.05, 0.1, 0.2**\n",
    "- `epochs`: try **30, 60, 100**\n",
    "- `h1/h2`: try **(3,3), (5,5), (10,10)**\n",
    "- `seed`: try different seeds to see variance\n",
    "\n",
    "Watch:\n",
    "- loss curve shape\n",
    "- training accuracy\n",
    "- whether predictions collapse to a single class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffeb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Experiment parameters =====\n",
    "h1, h2 = 3, 3\n",
    "lr = 0.1\n",
    "epochs = 30\n",
    "seed = 0\n",
    "\n",
    "scratch_model = TwoHiddenNN(h1=h1, h2=h2, seed=seed)\n",
    "losses = train_sgd(scratch_model, X_train, y_train, lr=lr, epochs=epochs, seed=seed)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"From-scratch SGD training loss (per step)\")\n",
    "plt.xlabel(\"SGD step\")\n",
    "plt.ylabel(\"Cross-entropy loss\")\n",
    "plt.show()\n",
    "\n",
    "final_weights_scratch = scratch_model.get_weights()\n",
    "{k: v.shape for k, v in final_weights_scratch.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624e54f",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Evaluation metrics + common warning explained\n",
    "\n",
    "You might see something like:\n",
    "\n",
    "```\n",
    "UndefinedMetricWarning: Precision is ill-defined ... due to no predicted samples\n",
    "```\n",
    "\n",
    "This happens when the model predicts **only one class** on the test set.\n",
    "\n",
    "### Why you might see Precision = Recall = F1 = 0.0\n",
    "\n",
    "If the scratch model predicts **only one class**, then:\n",
    "- Precision/recall for the *other* class is undefined.\n",
    "- `sklearn` warns you and sets that metric to 0 unless you choose `zero_division`.\n",
    "\n",
    "### What to try if this happens\n",
    "- Increase epochs (e.g. **60–100**)\n",
    "- Try a smaller learning rate (e.g. **0.05**)\n",
    "- Increase hidden sizes (e.g. **5** or **10** neurons)\n",
    "- Try different random seeds\n",
    "\n",
    "We use **macro averaging** so both classes are considered equally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_macro_metrics(y_true, y_pred):\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return prec, rec, f1\n",
    "\n",
    "yhat_test_scratch = scratch_model.predict(X_test)\n",
    "prec_s, rec_s, f1_s = eval_macro_metrics(y_test, yhat_test_scratch)\n",
    "\n",
    "print(\"From-scratch metrics on test (macro avg):\")\n",
    "print(\" precision:\", prec_s)\n",
    "print(\" recall   :\", rec_s)\n",
    "print(\" f1       :\", f1_s)\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_test, yhat_test_scratch))\n",
    "\n",
    "# Helpful diagnostic: how many predictions of each class?\n",
    "vals, counts = np.unique(yhat_test_scratch, return_counts=True)\n",
    "print(\"predicted class counts:\", dict(zip(vals, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34896cc4",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Framework comparison: PyTorch (same constraints)\n",
    "\n",
    "We build the same architecture in PyTorch:\n",
    "- 2 hidden sigmoid layers\n",
    "- no bias terms (`bias=False`)\n",
    "- 2 output logits\n",
    "\n",
    "Then we train with SGD and evaluate precision/recall/F1 the same way.\n",
    "\n",
    "> PyTorch’s `CrossEntropyLoss` expects **logits**, not probabilities, so we do **not** apply softmax in the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84944314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Keep everything in float64 to match NumPy more closely (optional)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "class TorchTwoHidden(nn.Module):\n",
    "    def __init__(self, d_in=2, h1=3, h2=3, d_out=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, h1, bias=False)\n",
    "        self.fc2 = nn.Linear(h1, h2, bias=False)\n",
    "        self.fc3 = nn.Linear(h2, d_out, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "\n",
    "# Tensors\n",
    "Xtr_t = torch.from_numpy(X_train)\n",
    "ytr_t = torch.from_numpy(y_train.astype(np.int64))\n",
    "Xte_t = torch.from_numpy(X_test)\n",
    "yte_t = torch.from_numpy(y_test.astype(np.int64))\n",
    "\n",
    "torch_model = TorchTwoHidden(h1=h1, h2=h2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(torch_model.parameters(), lr=lr)\n",
    "\n",
    "torch_losses = []\n",
    "n = X_train.shape[0]\n",
    "\n",
    "for ep in range(epochs):\n",
    "    perm = torch.randperm(n)\n",
    "    for i in perm:\n",
    "        xi = Xtr_t[i:i+1]\n",
    "        yi = ytr_t[i:i+1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = torch_model(xi)\n",
    "        loss = criterion(logits, yi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch_losses.append(loss.item())\n",
    "\n",
    "    if (ep + 1) % max(1, epochs // 5) == 0:\n",
    "        with torch.no_grad():\n",
    "            pred_train = torch.argmax(torch_model(Xtr_t), dim=1)\n",
    "            acc = (pred_train == ytr_t).double().mean().item()\n",
    "        print(f\"epoch {ep+1:3d}/{epochs} | last_step_loss={torch_losses[-1]:.4f} | train_acc={acc:.3f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch_losses)\n",
    "plt.title(\"PyTorch SGD training loss (per step)\")\n",
    "plt.xlabel(\"SGD step\")\n",
    "plt.ylabel(\"Cross-entropy loss\")\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat_test_torch = torch.argmax(torch_model(Xte_t), dim=1).cpu().numpy()\n",
    "\n",
    "prec_t, rec_t, f1_t = eval_macro_metrics(y_test, yhat_test_torch)\n",
    "\n",
    "print(\"PyTorch metrics on test (macro avg):\")\n",
    "print(\" precision:\", prec_t)\n",
    "print(\" recall   :\", rec_t)\n",
    "print(\" f1       :\", f1_t)\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_test, yhat_test_torch))\n",
    "\n",
    "vals, counts = np.unique(yhat_test_torch, return_counts=True)\n",
    "print(\"predicted class counts:\", dict(zip(vals, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b9724",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Side-by-side comparison\n",
    "\n",
    "This is the table you can paste into your homework write-up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame([\n",
    "    {\"model\": \"from_scratch_numpy\", \"precision\": prec_s, \"recall\": rec_s, \"f1\": f1_s},\n",
    "    {\"model\": \"pytorch_framework\",  \"precision\": prec_t, \"recall\": rec_t, \"f1\": f1_t},\n",
    "])\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc9534",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Final weights (scratch model)\n",
    "\n",
    "The homework asks you to report the final weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34faa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_weights_scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535a142",
   "metadata": {},
   "source": [
    "\n",
    "## Question to Consider\n",
    "\n",
    "If PyTorch does better than scratch, list two reasons besides “PyTorch is better.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91627346-95ab-4b39-a830-c04c1ce5dc16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
