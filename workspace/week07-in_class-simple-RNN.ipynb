{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3cf90-3ee5-46ac-aded-db452c5cfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    x = x - np.max(x)\n",
    "    ex = np.exp(x)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "def one_hot(i: int, V: int) -> np.ndarray:\n",
    "    x = np.zeros((V, 1), dtype=np.float64)\n",
    "    x[i, 0] = 1.0\n",
    "    return x\n",
    "\n",
    "# -----------------------------\n",
    "# Vanilla RNN (tanh) from scratch\n",
    "# -----------------------------\n",
    "class VanillaRNN:\n",
    "    def __init__(self, vocab_size: int, hidden_size: int = 64, seed: int = 0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # Parameters\n",
    "        # x_t is one-hot (Vx1), h_t is (Hx1)\n",
    "        self.Wxh = rng.normal(0, 0.01, size=(H, V))   # input -> hidden\n",
    "        self.Whh = rng.normal(0, 0.01, size=(H, H))   # hidden -> hidden\n",
    "        self.Why = rng.normal(0, 0.01, size=(V, H))   # hidden -> output (logits)\n",
    "        self.bh  = np.zeros((H, 1))\n",
    "        self.by  = np.zeros((V, 1))\n",
    "\n",
    "        # For Adagrad/SGD convenience (optional)\n",
    "        self._grads = None\n",
    "\n",
    "    def forward(self, inputs, hprev):\n",
    "        \"\"\"\n",
    "        inputs: list[int] of length T (token indices)\n",
    "        hprev: (H,1)\n",
    "        Returns: loss, cache\n",
    "        cache contains all intermediates for BPTT.\n",
    "        \"\"\"\n",
    "        V = self.by.shape[0]\n",
    "        H = self.bh.shape[0]\n",
    "        T = len(inputs)\n",
    "\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = hprev\n",
    "\n",
    "        for t in range(T):\n",
    "            xs[t] = one_hot(inputs[t], V)                       # (V,1)\n",
    "            hs[t] = np.tanh(self.Wxh @ xs[t] + self.Whh @ hs[t-1] + self.bh)  # (H,1)\n",
    "            ys[t] = self.Why @ hs[t] + self.by                 # (V,1) logits\n",
    "            ps[t] = softmax(ys[t])                              # (V,1)\n",
    "\n",
    "        cache = (xs, hs, ys, ps)\n",
    "        return cache\n",
    "\n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"\n",
    "        Cross-entropy over sequence.\n",
    "        targets: list[int] length T\n",
    "        \"\"\"\n",
    "        T = len(targets)\n",
    "        L = 0.0\n",
    "        for t in range(T):\n",
    "            p = ps[t][targets[t], 0]\n",
    "            L += -np.log(p + 1e-12)\n",
    "        return L / T\n",
    "\n",
    "    def bptt(self, cache, targets):\n",
    "        \"\"\"\n",
    "        Backprop through time to compute gradients.\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = cache\n",
    "        V = self.by.shape[0]\n",
    "        H = self.bh.shape[0]\n",
    "        T = len(targets)\n",
    "\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh  = np.zeros_like(self.bh)\n",
    "        dby  = np.zeros_like(self.by)\n",
    "\n",
    "        dh_next = np.zeros((H, 1))\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            # output gradient: dy = p - y_true\n",
    "            dy = ps[t].copy()\n",
    "            dy[targets[t], 0] -= 1.0                            # (V,1)\n",
    "\n",
    "            dWhy += dy @ hs[t].T                                # (V,H)\n",
    "            dby  += dy                                          # (V,1)\n",
    "\n",
    "            dh = self.Why.T @ dy + dh_next                      # (H,1)\n",
    "            dh_raw = (1 - hs[t] * hs[t]) * dh                   # tanh' = 1 - h^2\n",
    "\n",
    "            dbh  += dh_raw\n",
    "            dWxh += dh_raw @ xs[t].T                            # (H,V)\n",
    "            dWhh += dh_raw @ hs[t-1].T                          # (H,H)\n",
    "\n",
    "            dh_next = self.Whh.T @ dh_raw\n",
    "\n",
    "        # Clip to avoid exploding gradients\n",
    "        for dparam in (dWxh, dWhh, dWhy, dbh, dby):\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        self._grads = (dWxh, dWhh, dWhy, dbh, dby)\n",
    "        return self._grads\n",
    "\n",
    "    def step(self, lr=1e-1):\n",
    "        dWxh, dWhh, dWhy, dbh, dby = self._grads\n",
    "        self.Wxh -= lr * dWxh\n",
    "        self.Whh -= lr * dWhh\n",
    "        self.Why -= lr * dWhy\n",
    "        self.bh  -= lr * dbh\n",
    "        self.by  -= lr * dby\n",
    "\n",
    "    def sample(self, start_idx, n, h=None):\n",
    "        \"\"\"\n",
    "        Generate n characters starting from start_idx.\n",
    "        \"\"\"\n",
    "        V = self.by.shape[0]\n",
    "        H = self.bh.shape[0]\n",
    "        if h is None:\n",
    "            h = np.zeros((H, 1))\n",
    "\n",
    "        x = one_hot(start_idx, V)\n",
    "        out = [start_idx]\n",
    "        for _ in range(n - 1):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            y = self.Why @ h + self.by\n",
    "            p = softmax(y).ravel()\n",
    "            idx = int(np.random.choice(V, p=p))\n",
    "            out.append(idx)\n",
    "            x = one_hot(idx, V)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny demo: character LM\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data = \"hello Georgetown from scratch!\\n\"\n",
    "    chars = sorted(list(set(data)))\n",
    "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "    encoded = [stoi[ch] for ch in data]\n",
    "    V = len(chars)\n",
    "\n",
    "    rnn = VanillaRNN(vocab_size=V, hidden_size=32, seed=42)\n",
    "\n",
    "    seq_len = 12\n",
    "    lr = 0.2\n",
    "    h = np.zeros((32, 1))\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        # simple single-sequence training (wrap around)\n",
    "        start = epoch % (len(encoded) - seq_len - 1)\n",
    "        inp = encoded[start : start + seq_len]\n",
    "        tgt = encoded[start + 1 : start + seq_len + 1]\n",
    "\n",
    "        cache = rnn.forward(inp, h)\n",
    "        xs, hs, ys, ps = cache\n",
    "        L = rnn.loss(ps, tgt)\n",
    "\n",
    "        rnn.bptt(cache, tgt)\n",
    "        rnn.step(lr=lr)\n",
    "\n",
    "        # carry hidden state (detach by copying; here it's numpy so it's fine)\n",
    "        h = hs[seq_len - 1]\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            sample_ids = rnn.sample(start_idx=inp[0], n=40, h=h.copy())\n",
    "            sample_txt = \"\".join(itos[i] for i in sample_ids)\n",
    "            print(f\"epoch {epoch:3d} loss {L:.3f} | sample: {sample_txt!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebdb56-897b-4df4-8bf1-6bfc7eef688d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
