{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431265f-3a36-4eb0-998f-2427a1279acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny demo: character-level LM (next-char prediction) with Keras\n",
    "# -----------------------------\n",
    "data = \"hello Georgetown from scratch!\\n\" # would more data help? why or why not?\n",
    "chars = sorted(list(set(data)))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "V = len(chars)\n",
    "\n",
    "encoded = np.array([stoi[ch] for ch in data], dtype=np.int32)\n",
    "\n",
    "seq_len = 12 # would shorter/longer sequences help? why or why not?\n",
    "\n",
    "def make_dataset(encoded, seq_len, batch_size=32):\n",
    "    # Build (input_seq, target_seq) pairs, where target is next char for each time step\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(encoded) - seq_len):\n",
    "        x = encoded[i : i + seq_len]\n",
    "        y = encoded[i + 1 : i + seq_len + 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    xs = np.stack(xs)  # (N, T)\n",
    "    ys = np.stack(ys)  # (N, T)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((xs, ys))\n",
    "    return ds.shuffle(512).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds = make_dataset(encoded, seq_len, batch_size=16)\n",
    "\n",
    "# -----------------------------\n",
    "# Model: Embedding -> SimpleRNN -> Dense(vocab)\n",
    "# -----------------------------\n",
    "hidden_size = 32\n",
    "embed_dim = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(seq_len,), dtype=\"int32\"),\n",
    "    layers.Embedding(input_dim=V, output_dim=embed_dim),\n",
    "    layers.SimpleRNN(hidden_size, return_sequences=True, activation=\"tanh\"),\n",
    "    layers.Dense(V)  # logits over vocab at each time step\n",
    "])\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    loss=loss_fn,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.fit(ds, epochs=50, verbose=2)\n",
    "\n",
    "# -----------------------------\n",
    "# Sampling / generation\n",
    "# -----------------------------\n",
    "@tf.function\n",
    "def step_logits(model, x):\n",
    "    # x: (1, T) int32\n",
    "    logits = model(x, training=False)  # (1, T, V)\n",
    "    return logits[:, -1, :]            # (1, V) last-step logits\n",
    "\n",
    "def sample(model, start_char=\"h\", n=60, temperature=1.0):\n",
    "    start_idx = stoi[start_char]\n",
    "    # start with repeated start char so input is length seq_len\n",
    "    context = [start_idx] * seq_len\n",
    "    out = [start_idx]\n",
    "\n",
    "    for _ in range(n - 1):\n",
    "        x = tf.constant([context], dtype=tf.int32)  # (1, T)\n",
    "        logits = step_logits(model, x) / temperature\n",
    "        probs = tf.nn.softmax(logits).numpy().ravel()\n",
    "        next_idx = int(np.random.choice(V, p=probs))\n",
    "\n",
    "        out.append(next_idx)\n",
    "        context = context[1:] + [next_idx]\n",
    "\n",
    "    return \"\".join(itos[i] for i in out)\n",
    "\n",
    "print(sample(model, start_char=\"h\", n=80, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c6e89-b2c4-4ae3-873f-e9521943d3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
